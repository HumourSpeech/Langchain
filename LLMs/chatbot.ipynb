{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa76d986",
   "metadata": {},
   "source": [
    "<h2>Building A Chatbot </h2>\n",
    "Designing and implementing an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b4dae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be9db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nitin Mishra\\OneDrive\\Desktop\\AIML\\Gen-AI\\Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000002100A9C8980>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002100CC3C8C0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-120b\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef4ff5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Nitin! ðŸ‘‹ Great to meet you.\\n\\nItâ€™s exciting that youâ€™re diving into LangChainâ€”thereâ€™s a lot of fun (and powerful) stuff you can build with it. Howâ€™s your learning journey going so far? Are you:\\n\\n- Exploring the basic **chains** and **prompts**?\\n- Working on a **retrievalâ€‘augmented generation (RAG)** pipeline?\\n- Trying out **agents** that can call tools or APIs?\\n- Or something else entirely?\\n\\nIf youâ€™d like, I can share:\\n\\n1. **Quick starter resources** (official docs, tutorials, and a few handsâ€‘on notebooks).  \\n2. **Common pitfalls** to watch out for when setting up LLM wrappers, vector stores, or callbacks.  \\n3. **Project ideas** to cement what youâ€™ve learnedâ€”e.g., a personal knowledgeâ€‘base chatbot, a codeâ€‘assistant, or an automated research summarizer.  \\n\\nJust let me know where youâ€™re at or what youâ€™re curious about, and Iâ€™ll tailor the next steps for you! ðŸš€', additional_kwargs={'reasoning_content': 'We need to respond. The user says hi, introduces themselves, learning Langchain. Probably respond friendly, ask about progress, offer help, resources, etc. No policy issues.'}, response_metadata={'token_usage': {'completion_tokens': 264, 'prompt_tokens': 89, 'total_tokens': 353, 'completion_time': 0.633355846, 'completion_tokens_details': {'reasoning_tokens': 37}, 'prompt_time': 0.004490935, 'prompt_tokens_details': None, 'queue_time': 0.054319895, 'total_time': 0.637846781}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a28df4bce5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--c1b2fe73-d085-4403-86a7-40031970b237-0', usage_metadata={'input_tokens': 89, 'output_tokens': 264, 'total_tokens': 353, 'output_token_details': {'reasoning': 37}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17a37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Nitin\\u202fMishra**, and youâ€™re currently learning and working with the **LangChain framework**. If youâ€™d like any helpâ€”whether itâ€™s clarifying concepts, building a demo, or troubleshooting codeâ€”just let me know!', additional_kwargs={'reasoning_content': 'User asks: \"Hey What\\'s my name and what do I do?\" We have prior conversation: user introduced themselves as Nitin Mishra, learning Langchain. So answer: your name is Nitin Mishra, you are learning Langchain framework. Possibly also ask if they\\'d like help.'}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 160, 'total_tokens': 281, 'completion_time': 0.361758365, 'completion_tokens_details': {'reasoning_tokens': 59}, 'prompt_time': 0.006459446, 'prompt_tokens_details': None, 'queue_time': 0.054033394, 'total_time': 0.368217811}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_70d048ba3c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--80a09c08-5833-4a70-addc-56662304e793-0', usage_metadata={'input_tokens': 160, 'output_tokens': 121, 'total_tokens': 281, 'output_token_details': {'reasoning': 59}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrating a hard-coded conversation history to showcase how the model can maintain context.\n",
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.\"),\n",
    "        AIMessage(content=\"Hello Nitin Mishra! It's nice to meet you. \\n\\nAs someone learning the Langchain framework, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b9c24",
   "metadata": {},
   "source": [
    "<h3> Message History </h3>\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3225b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21749b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04a9fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56513fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Nitin! ðŸ‘‹ Great to hear youâ€™re diving into Langchain. Itâ€™s a powerful framework for building LLMâ€‘driven applications, and thereâ€™s a lot you can do with it.\\n\\nIf youâ€™d like, I can help with:\\n\\n* **Getting started** â€“ Quick setup guide, key concepts, and a minimal â€œHello, Worldâ€ example.\\n* **Core components** â€“ How to work with **Chains**, **Agents**, **Memory**, **PromptTemplates**, and **Callbacks**.\\n* **Integrations** â€“ Connecting Langchain to vector stores (FAISS, Pinecone, Chroma), document loaders, and external APIs.\\n* **Best practices** â€“ Structuring projects, handling token limits, managing retries, and testing.\\n* **Project ideas** â€“ Small to mediumâ€‘scale projects (Q&A bots, summarizers, retrievalâ€‘augmented generation, autonomous agents) you can build to solidify your knowledge.\\n\\nLet me know what youâ€™re focusing on right now or if thereâ€™s a particular piece thatâ€™s giving you trouble. Iâ€™m happy to walk through code snippets, explain concepts, or point you to helpful resources! ðŸš€'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943d54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hey Nitin! ðŸ‘‹ Great to hear youâ€™re diving into Langchain. Itâ€™s a powerful framework for building LLMâ€‘driven applications, and thereâ€™s a lot you can do with it.\\n\\nIf youâ€™d like, I can help with:\\n\\n* **Getting started** â€“ Quick setup guide, key concepts, and a minimal â€œHello, Worldâ€ example.\\n* **Core components** â€“ How to work with **Chains**, **Agents**, **Memory**, **PromptTemplates**, and **Callbacks**.\\n* **Integrations** â€“ Connecting Langchain to vector stores (FAISS, Pinecone, Chroma), document loaders, and external APIs.\\n* **Best practices** â€“ Structuring projects, handling token limits, managing retries, and testing.\\n* **Project ideas** â€“ Small to mediumâ€‘scale projects (Q&A bots, summarizers, retrievalâ€‘augmented generation, autonomous agents) you can build to solidify your knowledge.\\n\\nLet me know what youâ€™re focusing on right now or if thereâ€™s a particular piece thatâ€™s giving you trouble. Iâ€™m happy to walk through code snippets, explain concepts, or point you to helpful resources! ðŸš€', additional_kwargs={'reasoning_content': \"We need to respond. The user introduced themselves, says they're learning Langchain. We can greet, ask about their progress, offer help, resources, examples. Should be friendly.\"}, response_metadata={'token_usage': {'completion_tokens': 279, 'prompt_tokens': 89, 'total_tokens': 368, 'completion_time': 0.607111898, 'completion_tokens_details': {'reasoning_tokens': 37}, 'prompt_time': 0.003450717, 'prompt_tokens_details': None, 'queue_time': 0.053989503, 'total_time': 0.610562615}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ff6aa7708c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--ea7eadcd-b093-4c25-a7e3-cfb570d491d1-0', usage_metadata={'input_tokens': 89, 'output_tokens': 279, 'total_tokens': 368, 'output_token_details': {'reasoning': 37}})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"chat1\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c8bbf",
   "metadata": {},
   "source": [
    "Here is the step-by-step flow for a new session:\n",
    "\n",
    "1. The Trigger\n",
    "You call invoke with a specific session_id in the config:\n",
    "```t\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi...\")],\n",
    "    config={\"configurable\": {\"session_id\": \"chat1\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "2. The Lookup (Inside RunnableWithMessageHistory)\n",
    "Before sending anything to the LLM, the RunnableWithMessageHistory wrapper pauses and looks at your config. It sees session_id=\"chat1\".\n",
    "\n",
    "It then calls your helper function: get_session_history(\"chat1\").\n",
    "\n",
    "3. Creation and Storage (Inside get_session_history)\n",
    "This is where the storage happens.\n",
    "```t\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        # A. Create a new empty history object\n",
    "        new_history = ChatMessageHistory() \n",
    "        \n",
    "        # B. Save it into the dictionary immediately\n",
    "        store[session_id] = new_history \n",
    "        \n",
    "    return store[session_id]\n",
    "```\n",
    "- Crucial Step: The moment ChatMessageHistory() is assigned to store[session_id], the dictionary now holds a reference to that history object. Any changes made to that object later will be \"saved\" in the dictionary because the dictionary is holding the live object.\n",
    "\n",
    "\n",
    "4. The Update (Back inside RunnableWithMessageHistory)\n",
    "Now that the wrapper has the history object (let's call it history_obj), it does two things:\n",
    "\n",
    "- 1. Add User Message: It takes your input (\"Hi...\") and calls \n",
    "```history_obj.add_user_message(\"Hi...\").```\n",
    "    <br>Result: The object inside your store dictionary now contains [HumanMessage(content=\"Hi...\")].\n",
    "- 2. Call LLM: It sends the message to the LLM and gets a response (\"Hello!\").<br>\n",
    "- 3. Add AI Message: It takes the response and calls ```history_obj.add_ai_message(\"Hello!\").```<br>\n",
    "Result: The object inside your store dictionary now contains ```[HumanMessage(...), AIMessage(...)].```\n",
    "<h6>Summary</h6>\n",
    "The store isn't updated at the end; it is updated continuously.<br>\n",
    "\n",
    "Your function puts an empty container into the store.<br>\n",
    "The Runnable wrapper keeps adding messages into that container as the conversation flows.<br>\n",
    "Because store holds the\n",
    "\n",
    "container (the object), and the wrapper keeps filling that container, the store always has the latest messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f783933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Nitin\\u202fMishra**.', additional_kwargs={'reasoning_content': 'User asks again: \"What\\'s my name?\" Already answered: Nitin Mishra. Should respond accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 362, 'total_tokens': 405, 'completion_time': 0.093378925, 'completion_tokens_details': {'reasoning_tokens': 22}, 'prompt_time': 0.014964624, 'prompt_tokens_details': {'cached_tokens': 256}, 'queue_time': 0.053977056, 'total_time': 0.108343549}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_e88ce9c728', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--f322c0ed-8d30-4912-8f0f-fbab4dd51589-0', usage_metadata={'input_tokens': 362, 'output_tokens': 43, 'total_tokens': 405, 'input_token_details': {'cache_read': 256}, 'output_token_details': {'reasoning': 22}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af2bd7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Ria.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4516a481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Ria.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is Ria\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e05515e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Ria.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
