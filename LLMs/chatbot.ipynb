{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa76d986",
   "metadata": {},
   "source": [
    "<h2>Building A Chatbot </h2>\n",
    "Designing and implementing an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b4dae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be9db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nitin Mishra\\OneDrive\\Desktop\\AIML\\Gen-AI\\Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000018FC1772090>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000018FC17F9C10>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-120b\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef4ff5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Nitin! üëã Great to meet you, and it‚Äôs exciting that you‚Äôre diving into the LangChain framework. How‚Äôs the learning journey going so far? \\n\\nIf you‚Äôre looking for a quick roadmap or some handy resources, here are a few suggestions to help you get the most out of LangChain:\\n\\n---\\n\\n## 1Ô∏è‚É£ Core Concepts to Master\\n\\n| Concept | What It Is | Why It Matters |\\n|---------|------------|----------------|\\n| **Chains** | Sequentially connect LLM calls, prompts, and other components. | Lets you build complex workflows from simple building blocks. |\\n| **Agents** | LLMs that can decide which tool or chain to use next. | Enables dynamic, decision‚Äëmaking pipelines (e.g., ‚Äúsearch‚Äëthen‚Äësummarize‚Äù). |\\n| **Memory** | State‚Äëkeeping across calls (e.g., conversation history). | Makes interactions feel coherent and context‚Äëaware. |\\n| **Retrievers & Vector Stores** | Retrieve relevant documents from a knowledge base. | Powers RAG (Retrieval‚ÄëAugmented Generation) for factual accuracy. |\\n| **Tool Integration** | Connect external APIs, databases, or custom functions. | Extends the LLM‚Äôs capabilities beyond pure text generation. |\\n\\n---\\n\\n## 2Ô∏è‚É£ Hands‚ÄëOn Learning Path\\n\\n| Step | Activity | Resources |\\n|------|----------|-----------|\\n| **1Ô∏è‚É£ Install & Set Up** | `pip install langchain openai` (or other LLM provider). | Official docs: <https://python.langchain.com/docs/get_started/introduction> |\\n| **2Ô∏è‚É£ Play with Simple Chains** | Build a **LLMChain** that takes a prompt template and returns a response. | Quickstart notebook: <https://github.com/langchain-ai/langchain/blob/master/docs/get_started/quickstart.ipynb> |\\n| **3Ô∏è‚É£ Add Memory** | Use `ConversationBufferMemory` to keep chat history. | <https://python.langchain.com/docs/modules/memory/> |\\n| **4Ô∏è‚É£ Try Retrieval‚ÄëAugmented Generation** | Index a small set of documents with **FAISS** or **Chroma**, then query them. | RAG tutorial: <https://python.langchain.com/docs/use_cases/question_answering/> |\\n| **5Ô∏è‚É£ Build an Agent** | Create a **ZeroShotAgent** that can decide when to call a search tool. | Agent guide: <https://python.langchain.com/docs/modules/agents/> |\\n| **6Ô∏è‚É£ Connect a Real‚ÄëWorld Tool** | Wrap a simple API (e.g., weather, calculator) as a LangChain **Tool**. | Custom tool example: <https://python.langchain.com/docs/modules/agents/tools/> |\\n| **7Ô∏è‚É£ Deploy** | Wrap your chain/agent in a FastAPI or Flask app, or use LangServe for serverless endpoints. | Deployment docs: <https://python.langchain.com/docs/usage/serving/> |\\n| **8Ô∏è‚É£ Iterate & Optimize** | Experiment with prompt engineering, temperature settings, and different LLM providers (OpenAI, Anthropic, Llama‚Äë2, etc.). | Prompt engineering guide: <https://python.langchain.com/docs/how_to/prompt_engineering/> |\\n\\n---\\n\\n## 3Ô∏è‚É£ Mini‚ÄëProject Ideas\\n\\n1. **Personal Knowledge Base** ‚Äì Index your own notes (Markdown, PDFs) and ask natural‚Äëlanguage questions.  \\n2. **Chat‚ÄëBased Travel Planner** ‚Äì Use an agent that can call a flight‚Äësearch API, a hotel‚Äësearch API, and summarize itineraries.  \\n3. **Code Helper Bot** ‚Äì Combine a code‚Äëgeneration LLM with a tool that can run snippets in a sandbox and return results.  \\n4. **FAQ Bot for a Website** ‚Äì Pull existing FAQ pages into a vector store, then answer visitor questions in real time.\\n\\n---\\n\\n## 4Ô∏è‚É£ Tips & Gotchas\\n\\n| Tip | Explanation |\\n|-----|-------------|\\n| **Start Small** | Build a single‚Äëstep chain first; add memory or retrieval later. |\\n| **Version Pinning** | LangChain evolves quickly. Pin a version (`pip install \"langchain==0.1.*\"`) for reproducibility. |\\n| **Watch Token Usage** | Chains that call an LLM multiple times can quickly blow up token counts. Use `max_tokens` and `stop` wisely. |\\n| **Cache Retrievals** | If you‚Äôre hitting the same documents repeatedly, cache the embeddings to avoid recomputation. |\\n| **Logging** | Enable LangChain‚Äôs built‚Äëin logging (`langchain.debug = True`) to see the flow of calls. |\\n| **Community** | Join the LangChain Discord or GitHub Discussions for quick help and inspiration. |\\n\\n---\\n\\n## 5Ô∏è‚É£ Quick Code Snippet ‚Äì A Simple RAG Chat\\n\\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.document_loaders import TextLoader\\n\\n# 1Ô∏è‚É£ Load & index docs\\nloader = TextLoader(\"my_notes.txt\")\\ndocs = loader.load()\\nembeddings = OpenAIEmbeddings()\\nvector_store = FAISS.from_documents(docs, embeddings)\\n\\n# 2Ô∏è‚É£ Prompt template\\nprompt = PromptTemplate(\\n    input_variables=[\"question\", \"context\"],\\n    template=\"\"\"\\nYou are an assistant that answers questions using the provided context.\\nIf the context does not contain enough information, say \"I don\\'t know.\"\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\"\"\\n)\\n\\n# 3Ô∏è‚É£ LLM chain\\nllm = OpenAI(temperature=0)\\nchain = LLMChain(llm=llm, prompt=prompt)\\n\\n# 4Ô∏è‚É£ Memory (optional)\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\\n\\ndef rag_chat(user_query: str):\\n    # Retrieve relevant docs\\n    docs = vector_store.similarity_search(user_query, k=3)\\n    context = \"\\\\n\".join([d.page_content for d in docs])\\n\\n    # Generate answer\\n    answer = chain.run({\"question\": user_query, \"context\": context})\\n    memory.save_context({\"input\": user_query}, {\"output\": answer})\\n    return answer\\n\\n# Example usage\\nprint(rag_chat(\"What are the key steps for deploying a LangChain app?\"))\\n```\\n\\nFeel free to copy‚Äëpaste, tweak the prompt, or swap in a different LLM provider.\\n\\n---\\n\\n## 6Ô∏è‚É£ Where to Ask Questions\\n\\n- **LangChain Discord** ‚Äì #help channel is very responsive.  \\n- **Stack Overflow** ‚Äì Tag your question with `langchain`.  \\n- **GitHub Issues** ‚Äì Good for bugs or feature requests.  \\n\\n---\\n\\n### üéâ Your Next Move\\n\\nPick one of the mini‚Äëprojects above (or something of your own), follow the step‚Äëby‚Äëstep path, and let me know how it goes. If you hit a snag or want deeper dives into any component (e.g., custom agents, advanced memory patterns), just shout‚Äîhappy to help!\\n\\nHappy chaining! üöÄ', additional_kwargs={'reasoning_content': 'The user says \"Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.\" Probably they want a response, maybe ask about what they need help with. As an assistant, we respond friendly, ask about their progress, maybe provide resources, tips. No disallowed content. So we can respond.'}, response_metadata={'token_usage': {'completion_tokens': 1553, 'prompt_tokens': 89, 'total_tokens': 1642, 'completion_time': 3.241357208, 'completion_tokens_details': {'reasoning_tokens': 67}, 'prompt_time': 0.003505821, 'prompt_tokens_details': None, 'queue_time': 0.054915559, 'total_time': 3.2448630290000002}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8a618bed98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--58ae53b3-eba2-4514-ab26-265c0a06dba1-0', usage_metadata={'input_tokens': 89, 'output_tokens': 1553, 'total_tokens': 1642, 'output_token_details': {'reasoning': 67}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a17a37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Nitin\\u202fMishra**, and you‚Äôre currently **learning the LangChain framework**.', additional_kwargs={'reasoning_content': 'The user asks: \"Hey What\\'s my name and what do I do?\" We have context: earlier user said \"Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.\" So answer: name Nitin Mishra, they are learning Langchain framework. Probably respond.'}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 160, 'total_tokens': 253, 'completion_time': 0.19922749, 'completion_tokens_details': {'reasoning_tokens': 61}, 'prompt_time': 0.006511732, 'prompt_tokens_details': None, 'queue_time': 0.049616918, 'total_time': 0.205739222}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_e10890e4b9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--da948aed-0ec7-4eec-a339-5ba8a957252c-0', usage_metadata={'input_tokens': 160, 'output_tokens': 93, 'total_tokens': 253, 'output_token_details': {'reasoning': 61}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrating a hard-coded conversation history to showcase how the model can maintain context.\n",
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.\"),\n",
    "        AIMessage(content=\"Hello Nitin Mishra! It's nice to meet you. \\n\\nAs someone learning the Langchain framework, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b9c24",
   "metadata": {},
   "source": [
    "<h3> Message History </h3>\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3225b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21749b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a9fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56513fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Nitin! üëã\\n\\nThat‚Äôs great to hear you‚Äôre diving into LangChain. It‚Äôs a powerful framework for building LLM‚Äëdriven applications, and there‚Äôs a lot you can do with it‚Äîfrom simple chatbots to complex retrieval‚Äëaugmented generation pipelines.\\n\\nIf you‚Äôd like, I can help with:\\n\\n- **Getting started** ‚Äì a quick ‚Äúhello‚Äëworld‚Äù example and the typical project layout.  \\n- **Core concepts** ‚Äì chains, agents, memory, callbacks, and how they fit together.  \\n- **Integrations** ‚Äì connecting to vector stores (FAISS, Pinecone, Chroma, etc.), document loaders, or external APIs.  \\n- **Best practices** ‚Äì structuring prompts, handling errors, testing, and deployment tips.  \\n- **Resources** ‚Äì tutorials, official docs, community projects, and sample repos.\\n\\nLet me know what you‚Äôre most interested in or any specific question you have, and we can dive right in! üöÄ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5943d54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi , My name is Nitin Mishra. Currently I am learning Langchain framework.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Nitin! üëã\\n\\nThat‚Äôs great to hear you‚Äôre diving into LangChain. It‚Äôs a powerful framework for building LLM‚Äëdriven applications, and there‚Äôs a lot you can do with it‚Äîfrom simple chatbots to complex retrieval‚Äëaugmented generation pipelines.\\n\\nIf you‚Äôd like, I can help with:\\n\\n- **Getting started** ‚Äì a quick ‚Äúhello‚Äëworld‚Äù example and the typical project layout.  \\n- **Core concepts** ‚Äì chains, agents, memory, callbacks, and how they fit together.  \\n- **Integrations** ‚Äì connecting to vector stores (FAISS, Pinecone, Chroma, etc.), document loaders, or external APIs.  \\n- **Best practices** ‚Äì structuring prompts, handling errors, testing, and deployment tips.  \\n- **Resources** ‚Äì tutorials, official docs, community projects, and sample repos.\\n\\nLet me know what you‚Äôre most interested in or any specific question you have, and we can dive right in! üöÄ', additional_kwargs={'reasoning_content': 'The user says hi, introduces themselves, mentions they are learning Langchain. Likely wants to discuss or ask something. We should respond friendly, ask how can help, maybe give resources, suggestions. No disallowed content. So respond accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 259, 'prompt_tokens': 89, 'total_tokens': 348, 'completion_time': 0.549898824, 'completion_tokens_details': {'reasoning_tokens': 50}, 'prompt_time': 0.003429001, 'prompt_tokens_details': None, 'queue_time': 0.049903929, 'total_time': 0.553327825}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8a618bed98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--ea3075e5-134d-4d6d-9483-4cc3dfdc02d1-0', usage_metadata={'input_tokens': 89, 'output_tokens': 259, 'total_tokens': 348, 'output_token_details': {'reasoning': 50}})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"chat1\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c8bbf",
   "metadata": {},
   "source": [
    "Here is the step-by-step flow for a new session:\n",
    "\n",
    "1. The Trigger:-<br>\n",
    "You call invoke with a specific session_id in the config:\n",
    "```t\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi...\")],\n",
    "    config={\"configurable\": {\"session_id\": \"chat1\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "2. The Lookup (Inside RunnableWithMessageHistory):-<br>\n",
    "Before sending anything to the LLM, the RunnableWithMessageHistory wrapper pauses and looks at your config. It sees session_id=\"chat1\".\n",
    "\n",
    "It then calls your helper function: get_session_history(\"chat1\").\n",
    "\n",
    "3. Creation and Storage (Inside get_session_history):-<br>\n",
    "This is where the storage happens.\n",
    "```t\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        # A. Create a new empty history object\n",
    "        new_history = ChatMessageHistory() \n",
    "        \n",
    "        # B. Save it into the dictionary immediately\n",
    "        store[session_id] = new_history \n",
    "        \n",
    "    return store[session_id]\n",
    "```\n",
    "- Crucial Step: The moment ChatMessageHistory() is assigned to store[session_id], the dictionary now holds a reference to that history object. Any changes made to that object later will be \"saved\" in the dictionary because the dictionary is holding the live object.\n",
    "\n",
    "\n",
    "4. The Update (Back inside RunnableWithMessageHistory)<br>\n",
    "Now that the wrapper has the history object (let's call it history_obj), it does two things:\n",
    "\n",
    "- 1. Add User Message: It takes your input (\"Hi...\") and calls \n",
    "```history_obj.add_user_message(\"Hi...\").```\n",
    "    <br>Result: The object inside your store dictionary now contains [HumanMessage(content=\"Hi...\")].\n",
    "- 2. Call LLM: It sends the message to the LLM and gets a response (\"Hello!\").<br>\n",
    "- 3. Add AI Message: It takes the response and calls ```history_obj.add_ai_message(\"Hello!\").```<br>\n",
    "Result: The object inside your store dictionary now contains ```[HumanMessage(...), AIMessage(...)].```\n",
    "<h6>Summary</h6>\n",
    "The store isn't updated at the end; it is updated continuously.<br>\n",
    "\n",
    "Your function puts an empty container into the store.<br>\n",
    "The Runnable wrapper keeps adding messages into that container as the conversation flows.<br>\n",
    "Because store holds the\n",
    "\n",
    "container (the object), and the wrapper keeps filling that container, the store always has the latest messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f783933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Nitin\\u202fMishra**.', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" The user earlier said \"Hi , My name is Nitin Mishra.\" So answer: Nitin Mishra. Should be concise.'}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 303, 'total_tokens': 360, 'completion_time': 0.117794174, 'completion_tokens_details': {'reasoning_tokens': 36}, 'prompt_time': 0.013201416, 'prompt_tokens_details': None, 'queue_time': 0.054650153, 'total_time': 0.13099559}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--f3b21bff-4d0f-4aad-ba6f-25f22249c361-0', usage_metadata={'input_tokens': 303, 'output_tokens': 57, 'total_tokens': 360, 'output_token_details': {'reasoning': 36}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2bd7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don‚Äôt have any information about your name. If you‚Äôd like me to address you a certain way, just let me know!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4516a481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Ria! How can I help you today?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is Ria\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e05515e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Ria.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784557fb",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3237aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a5e16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Nikhil! üëã Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi My name is Nikhil Mishra.\" Probably wants a greeting. We can respond politely, ask how we can help. Also maybe ask if they\\'d like to share more. Use friendly tone.'}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 100, 'total_tokens': 173, 'completion_time': 0.154825083, 'completion_tokens_details': {'reasoning_tokens': 45}, 'prompt_time': 0.003775973, 'prompt_tokens_details': None, 'queue_time': 0.053991677, 'total_time': 0.158601056}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_626f3fc5e0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--3d33a8cb-e6b4-4677-8668-785f1be3b2fd-0', usage_metadata={'input_tokens': 100, 'output_tokens': 73, 'total_tokens': 173, 'output_token_details': {'reasoning': 45}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Nikhil Mishra.\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51cbff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c4ee34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Nikhil! üëã Nice to meet you. How can I assist you today?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Nikhil Mishra.\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ead22ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Nikhil Mishra.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4e4856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33710a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§Ü‡§¶‡§∞‡•ç‡§∂! ‡§Ü‡§™‡§∏‡•á ‡§Æ‡§ø‡§≤‡§ï‡§∞ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•Å‡§à‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡•à‡§∏‡•á ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Adarsh\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d1c4f",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd62f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34b083b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§®‡§ø‡§§‡§ø‡§® ‡§Æ‡§ø‡§∂‡•ç‡§∞‡§æ ‡§ú‡•Ä! ‡§Ü‡§™‡§ï‡§æ ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡§ø‡§∏ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Nitin Mishra\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87edd49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§®‡§ø‡§§‡§ø‡§® ‡§Æ‡§ø‡§∂‡•ç‡§∞‡§æ ‡§π‡•à‡•§'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbfb8d",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
